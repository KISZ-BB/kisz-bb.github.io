# AI Maker Community - Weekly Paper Readings

* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

* [Algorithms for Hyper-parameter Optimization](https://papers.nips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html)

* [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

* [Attention is All You Need](https://arxiv.org/abs/1706.03762)

* [Auto-encoding Variational Bayes](https://arxiv.org/abs/1312.6114)

* [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)

* [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf)

* [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

* [Binarized Neural Networks](https://arxiv.org/abs/1602.02830)

* [Communication-Efficient Learning of Deep Networks from Decentralized Data](https://arxiv.org/abs/1602.05629)

* [Confident Learning: Estimating Uncertainty in Dataset Labels](https://arxiv.org/abs/1911.00068)

* [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)

* [Deep Residual Networks for Image Recognition](https://arxiv.org/abs/1512.03385)

* [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

* [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142)

* [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)

* [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)

* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)

* [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)

* [Genie: Generative Interactive Environments](https://arxiv.org/abs/2402.15391)

* [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)

* [Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/abs/1812.08434v6)

* [Hidden Technical Debt in Machine Learning Systems](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)

* [High Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

* [ImageNet Classification with Deep Convolutional Networks](https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)

* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

* [Learning Deep Features for Discriminative Localization](https://arxiv.org/abs/1512.04150)

* [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

* [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)

* [LLama2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)

* [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

* [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)

* [Long-form Factuality in Large Language Models](https://arxiv.org/abs/2403.18802)

* [MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution](https://arxiv.org/abs/2403.17927)

* [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752)

* [Mask R-CNN](https://arxiv.org/abs/1703.06870)

* [No-Free Lunch Theorems for Optimization](https://www.cs.ubc.ca/~hutter/earg/papers07/00585893.pdf)

* [Panoptic Segmentation](https://arxiv.org/abs/1801.00868)

* [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)

* [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

* [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)

* [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)

* [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)

* [Segment Anything](https://arxiv.org/abs/2304.02643)

* [The Algorithmic Foundations of Differential Privacy](https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf)

* [The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective](https://link.springer.com/article/10.3758/s13423-016-1221-4)

* [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)

* [The M6 forecasting competition: Bridging the gap between forecasting and investment decisions](https://arxiv.org/abs/2310.13357)

* [The Matrix Calculus You Need for Deep Learning](https://arxiv.org/abs/1802.01528)

* [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)

* [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)

* [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)

* [When Do Neural Nets Outperform Boosted Trees on Tabular Data?](https://arxiv.org/abs/2305.02997)
