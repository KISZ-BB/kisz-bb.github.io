# Weekly Paper Readings

Deep Residual Networks for Image Recognition 
https://arxiv.org/abs/1512.03385

Attention is All You Need
https://arxiv.org/abs/1706.03762

ImageNet Classification with Deep Convolutional Networks
https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html

Learning Deep Features for Discriminative Localization 
https://arxiv.org/abs/1512.04150

Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization 
https://arxiv.org/abs/1610.02391

Axiomatic Attribution for Deep Networks
https://arxiv.org/abs/1703.01365

Visualizing and Understanding Convolutional Networks 
https://arxiv.org/abs/1311.2901

Reinventing RNNs for the Transformer Era
https://arxiv.org/abs/2305.13048

Long-form Factuality in Large Language Models
https://arxiv.org/abs/2403.18802

 Learning Transferable Visual Models From Natural Language Supervision
 https://arxiv.org/abs/2103.00020

 Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning
 https://arxiv.org/abs/1506.02142

U-Net: Convolutional Networks for Biomedical Image Segmentation 
https://arxiv.org/abs/1505.04597

Cyclical Learning Rates for Training Neural Networks
https://arxiv.org/abs/1506.01186

Learning Transferable Visual Models From Natural Language Supervision
https://arxiv.org/abs/2103.00020

RWKV: Reinventing RNNs for the Transformer Era
https://arxiv.org/abs/2305.13048

Confident Learning: Estimating Uncertainty in Dataset Labels
https://arxiv.org/abs/1911.00068

LLama2: Open Foundation and Fine-Tuned Chat Models
https://arxiv.org/abs/2307.09288

Segment Anything
https://arxiv.org/abs/2304.02643

Focal Loss for Dense Object Detection
https://arxiv.org/abs/1708.02002

Bag of Tricks for Image Classification with Convolutional Neural Networks
https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks_CVPR_2019_paper.pdf

Hidden Technical Debt in Machine Learning Systems
https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf

High Resolution Image Synthesis with Latent Diffusion Models
https://arxiv.org/abs/2112.10752

ReAct: Synergizing Reasoning and Acting in Language Models
https://arxiv.org/abs/2210.03629

Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks
https://arxiv.org/abs/2005.11401

MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution
https://arxiv.org/abs/2403.17927

Mask R-CNN
https://arxiv.org/abs/1703.06870

The M6 forecasting competition: Bridging the gap between forecasting and investment decisions
https://arxiv.org/abs/2310.13357

Panoptic Segmentation
https://arxiv.org/abs/1801.00868

When Do Neural Nets Outperform Boosted Trees on Tabular Data?
https://arxiv.org/abs/2305.02997

LoRA: Low-Rank Adaptation of Large Language Models
https://arxiv.org/abs/2106.09685

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
https://arxiv.org/abs/1810.04805

LightGBM: A Highly Efficient Gradient Boosting Decision Tree
https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html

Dropout: A Simple Way to Prevent Neural Networks from Overfitting
https://jmlr.org/papers/v15/srivastava14a.html

Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
https://arxiv.org/abs/1502.03167

Long Short-Term Memory
https://www.bioinf.jku.at/publications/older/2604.pdf

Algorithms for Hyper-parameter Optimization
https://papers.nips.cc/paper_files/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html

QLoRA: Efficient Finetuning of Quantized LLMs
https://arxiv.org/abs/2305.14314

Training Compute-Optimal Large Language Models
https://arxiv.org/abs/2203.15556

Generative Adversarial Networks
https://arxiv.org/abs/1406.2661

Binarized Neural Networks
https://arxiv.org/abs/1602.02830

Playing Atari with Deep Reinforcement Learning
https://arxiv.org/abs/1312.5602

Auto-encoding Variational Bayes
https://arxiv.org/abs/1312.6114

Direct Preference Optimization: Your Language Model is Secretly a Reward Model
https://arxiv.org/abs/2305.18290

The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
https://arxiv.org/abs/2402.17764

Mamba: Linear-Time Sequence Modeling with Selective State Spaces
https://arxiv.org/abs/2312.00752

Genie: Generative Interactive Environments
https://arxiv.org/abs/2402.15391

An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
https://arxiv.org/abs/2010.11929

The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective
https://link.springer.com/article/10.3758/s13423-016-1221-4

Communication-Efficient Learning of Deep Networks from Decentralized Data
https://arxiv.org/abs/1602.05629

The Algorithmic Foundations of Differential Privacy
https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf

The Matrix Calculus You Need for Deep Learning
https://arxiv.org/abs/1802.01528

FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
https://arxiv.org/abs/2205.14135

No-Free Lunch Theorems for Optimization
https://www.cs.ubc.ca/~hutter/earg/papers07/00585893.pdf

A Formal Theory of Inductive Inference
https://www.sciencedirect.com/science/article/pii/S0019995864902232

Adam: A Method for Stochastic Optimization
https://arxiv.org/abs/1412.6980

Language Models are Few-Shot Learners
https://arxiv.org/abs/2005.14165

Graph Neural Networks: A Review of Methods and Applications
https://arxiv.org/abs/1812.08434v6

A Formal Theory of Inductive Inference
https://www.sciencedirect.com/science/article/pii/S0019995864902232

A Collection of Definitions of Intelligence
https://arxiv.org/abs/0706.3639

NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis
https://arxiv.org/abs/2003.08934
